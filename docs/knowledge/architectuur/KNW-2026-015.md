# KNW-2026-015: Altijd een limiet op collection endpoints om memory exhaustion te voorkomen

## Metadata
- **Categorie:** architectuur
- **Relevantie:** hoog
- **Datum:** 2026-02-28
- **Sessie-context:** D4 Export feature — review finding PRF-001

## Inzicht

API-endpoints die collecties teruggeven moeten altijd een limiet hebben, ook als de dataset nu klein is. Zonder limiet groeit het geheugengebruik lineair met de data en kan een enkel request de server platleggen. Gebruik een `limit` parameter met een veilige default en een hard maximum.

## Context

`api_export_project` laadde alle sessies voor een project in één keer in memory en stuurde ze als één response terug. Bij een project met duizenden sessies kon dit het serverproces laten crashen. Fix: `limit: int = Query(100, ge=1, le=500)` met `sessions = sessions[:limit]`.

## Geleerd

- **Default + hard max:** Een default van 100 met een maximum van 500 is een pragmatische keuze: voldoende voor normale use cases, beschermd tegen misbruik.
- **Slice na laden is de simpelste fix**, maar bij grote datasets is database-level LIMIT + OFFSET efficiënter.
- **Dit geldt voor alle list/export endpoints**, niet alleen dit specifieke geval.
- **Denk aan groei:** Een dataset die nu 10 items heeft, kan over een jaar 10.000 items bevatten.

## Toepassing

- Elke API die een lijst retourneert: sessies, gebruikers, logs, exports.
- Batch-operaties die alle records verwerken.
- CLI-commands die output naar stdout sturen (unbounded output kan pipes blokkeren).
- Defensief programmeren: ga uit van worst-case datavolume, niet van huidige staat.
